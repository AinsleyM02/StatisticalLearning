---
title: "Homework #5: Probability and Classification" 
author: "**Ainsley McLaughlin**"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```


# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
dir_data= 'https://mdporter.github.io/teaching/data/' # data directory
library(glmnet)
library(tidyverse) # functions for data manipulation  
```


# Crime Linkage

Crime linkage attempts to determine if a set of unsolved crimes share a common offender. *Pairwise* crime linkage is the more simple task of deciding if two crimes share a common offender; it can be considered a binary classification problem. The linkage training data has 8 evidence variables that measure the similarity between a pair of crimes:

- `spatial` is the spatial distance between the crimes
- `temporal` is the fractional time (in days) between the crimes
- `tod` and `dow` are the differences in time of day and day of week between the crimes
- `LOC`, `POA,` and `MOA` are binary with a 1 corresponding to a match (type of property, point of entry, method of entry)
- `TIMERANGE` is the time between the earliest and latest possible times the crime could have occurred (because the victim was away from the house during the crime).
- The response variable indicates if the crimes are linked ($y=1$) or unlinked ($y=0$).


These problems use the [linkage-train](`r file.path(dir_data, "linkage_train.csv") `) and [linkage-test](`r file.path(dir_data, "linkage_test.csv") `) datasets (click on links for data). 


## Load Crime Linkage Data

::: {.callout-note title="Solution"}
```{r}
#load the data
cld_train<-read.csv("hw5_linkage_train.csv", header=T)
cld_test<-read.csv("hw5_linkage_test.csv", header=T)

head(cld_test)

```
:::

# Problem 1: Penalized Regression for Crime Linkage

## a. Fit a penalized *linear regression* model to predict linkage. 

Use an elastic net penalty (including lasso and ridge) (your choice). 

- Report the value of $\alpha \in [0, 1]$ used. 
- Report the value of $\lambda$ used.
- Report the estimated coefficients.

::: {.callout-note title="Solution"}
```{r}
#scale the data, it is already dummy encoded, eg no categoricals with labels
cld_train_s<-data.frame((cld_train)) # this has a y
cld_test_s<-data.frame((cld_test)) # this has no y

```


```{r}
# import 
library(glmnet)
# Set the seed for reproducibility
set.seed(123)

# A list of alphas to search over
alphas <- seq(0, 1, by = 0.05)

# Number of folds for cross-validation
numfolds <- 10

# Create fold IDs for cross-validation
fold <- sample(rep(1:numfolds, length = nrow(cld_train_s)))

# Initialize an empty dataframe to store the alphas, lambda, and RMSE values
df_a_rmse <- data.frame(alpha = alphas, lambda = rep(NA, length(alphas)), rmse = rep(NA, length(alphas)))

# Loop through each alpha and fit the model for each one
for (i in seq_along(alphas)) {
  a <- alphas[i]
  df_a_rmse$alpha[i] <- a
  
  # Fit the elastic net model with cross-validation for this alpha
  fit_enet <- cv.glmnet(as.matrix(cld_train_s[ , -9]),  # Use only predictors (exclude 'y')
                        cld_train_s$y, 
                        alpha = a, 
                        foldid = fold)
  
  # Get the lambda that minimizes the mean squared error
  lambda_min <- fit_enet$lambda.min
  
  # Get predictions for this model using the lambda.min
  predicted_y <- predict(fit_enet, newx = as.matrix(cld_train_s[ ,-which(names(cld_train_s) == "y")]), s = "lambda.min")
  
  # Calculate RMSE for this alpha
  rmse_val <- sqrt(mean((cld_train_s$y - predicted_y)^2))
  
  # Save RMSE and lambda for this alpha
  df_a_rmse$rmse[i] <- rmse_val
  df_a_rmse$lambda[i] <- lambda_min
}

# Find the alpha with the minimum RMSE
min_a <- df_a_rmse$alpha[which.min(df_a_rmse$rmse)]

# Get the lambda associated with the minimum RMSE
lambda_best_a <- df_a_rmse$lambda[which.min(df_a_rmse$rmse)]

# Refit the model using the best alpha and lambda
fit_glmnet_enet <- glmnet(as.matrix(cld_train_s[, -which(names(cld_train_s) == "y")]), 
                          cld_train_s$y, 
                          alpha = min_a, 
                          lambda = lambda_best_a)

# Make predictions on the train data (unconvential, but per instructions) using the best alpha and lambda
yhat_enet_linear <- predict(fit_glmnet_enet, newx = as.matrix(cld_train_s[,-9]), s = lambda_best_a)

# Get the minimum RMSE from the training phase
min_rmse_val <- min(df_a_rmse$rmse)

# Print the best alpha and lambda associated with the lowest RMSE
print(paste("The alpha associated with the model with the lowest RMSE value (", min_rmse_val, ") is: ", min_a, 
            " and the minimum lambda at that alpha found by cv.glmnet is ", lambda_best_a))


```

```{r}
# Print the coefficients for the final model
coef(fit_glmnet_enet)


```
:::


## b. Fit a penalized *logistic regression* model to predict linkage. 

Use an elastic net penalty (including lasso and ridge) (your choice). 

- Report the value of $\alpha \in [0, 1]$ used. # could pick arbitrarily
- Report the value of $\lambda$ used. # could pick arbitrarily
- Report the estimated coefficients.

Notes: Logistic regression uses log loss instead of rmse to evaluate the best alpha

::: {.callout-note title="Solution"}
```{r}
# Load necessary library
library(glmnet)

# Set the seed for reproducibility
set.seed(123)

# A list of alphas to search over
alphas <- seq(0, 1, by = 0.05)

# Number of folds for cross-validation
numfolds <- 10

# Create fold IDs for cross-validation
fold <- sample(rep(1:numfolds, length = nrow(cld_train)))

# Initialize an empty dataframe to store the alphas, lambda, and log loss values
df_a_logloss <- data.frame(alpha = alphas, lambda = rep(NA, length(alphas)), log_loss = rep(NA, length(alphas)))

# Loop through each alpha and fit the model for each one
for (i in seq_along(alphas)) {
  a <- alphas[i]
  df_a_logloss$alpha[i] <- a
  
  # Fit the logistic elastic net model with cross-validation for this alpha
  fit_enet <- cv.glmnet(as.matrix(cld_train[, -which(names(cld_train) == "y")]), 
                        cld_train$y, 
                        alpha = a, 
                        foldid = fold, 
                        family = "binomial")
  
  # Get the lambda that minimizes the log loss
  lambda_min <- fit_enet$lambda.min
  
  # Get predictions (probabilities) for this model using the lambda.min
  predicted_probs <- predict(fit_enet, newx = as.matrix(cld_train[, -which(names(cld_train) == "y")]), s = "lambda.min", type = "response")
  
  # Calculate log loss for the model
  log_loss_val <- -mean(cld_train$y * log(predicted_probs) + (1 - cld_train$y) * log(1 - predicted_probs))
  
  # Save log loss and lambda for this alpha
  df_a_logloss$log_loss[i] <- log_loss_val
  df_a_logloss$lambda[i] <- lambda_min
}

# Find the alpha with the minimum log loss
min_a <- df_a_logloss$alpha[which.min(df_a_logloss$log_loss)]

# Get the lambda associated with the minimum log loss
lambda_best_a <- df_a_logloss$lambda[which.min(df_a_logloss$log_loss)]

# Refit the model using the best alpha and lambda
fit_glmnet_enet <- glmnet(as.matrix(cld_train[, -which(names(cld_train) == "y")]), cld_train$y, alpha = min_a, lambda = lambda_best_a, family = "binomial")

# Get the estimated coefficients
estimated_coefficients <- coef(fit_glmnet_enet, s = lambda_best_a)

# Make predictions on the train data (unconvential, but per instruction)
yhat_probs_logistic <- predict(fit_glmnet_enet, newx = as.matrix(cld_train_s[,-9]), s = lambda_best_a, type = "response")

# Print the best alpha, lambda, and estimated coefficients
print(paste("The alpha associated with the model with the lowest log loss is: ", min_a))
print(paste("The minimum lambda at that alpha is: ", lambda_best_a))
print("Estimated coefficients:")
print(estimated_coefficients)

```
:::

# Problem 2: Random Forest for Crime Linkage

Fit a random forest model to predict crime linkage.

Notes:
1.5.4 in the trees
4.2-4.4 helpful for random forest tuning and variable importance
prob modeling 3.4 for binary classification loss functions

- Report the loss function (or splitting rule) used. # pick arbitrarily
- Report any non-default tuning parameters.
- Report the variable importance (indicate which importance method was used). 

::: {.callout-note title="Solution"}
```{r}
help(ranger)
```

```{r}
# make y a factor for gini (loss function)
# or can use the brier score default (loss function) 
library(ranger)
library(modeldata)

# Fit the random forest model using ranger on the training data
rf_model <- ranger(y ~ .-y, data = cld_train_s, 
                     num.trees = 10, mtry = 3, 
                     importance = 'none')
  
# Make predictions on the training set also per the instructions-not typical, usually do it on the test set
y_hat_rforest <- predict(rf_model, cld_train_s[,-9])$predictions

  



```
:::

# Problem 3: ROC Curves

## a. ROC curve: training data

Produce one plot that has the ROC curves, using the *training data*, for all three models (linear, logistic, and random forest). Use color and/or linetype to distinguish between models and include a legend.    
Also report the AUC (area under the ROC curve) for each model. Again, use the *training data*. 

- Note: you should be weary of being asked to evaluation predictive performance from the same data used to estimate the tuning and model parameters. The next problem will walk you through a more proper way of evaluating predictive performance with resampling. 

::: {.callout-note title="Solution"}
```{r}
# in the classification notes, pg 22 or pg 15, is looking at gamma not predictions, so swap that out
# can use that new things
# put sensititivty or specificity on the x axis
# yardstick with, the default 

# can try using as.vector around the predicts

test_perf =
tibble(
truth = Default$y[test.ind] %>% factor(levels=c(1,0)),
gamma_hat
)

library(yardstick) # for evaluation functions
#: The yardstick package requires the categorical outcome be
# a factor with 1st level the outcome of interest.
test_perf =
tibble(
  truth = as.factor(cld_train_s$y), 
  yhat_enet_linear, 
  yhat_probs_logistic,
  y_hat_rforest 
)
#: ROC plots
ROC =
test_perf %>%
yardstick::roc_curve(truth,yhat_enet_linear, 
  yhat_probs_logistic,
  y_hat_rforest )

```
```{r}
# Load necessary libraries
library(yardstick)  # For evaluation functions
library(ggplot2)    # For plotting
library(dplyr)      # For data manipulation
library(tidyr)      # For pivot_longer

# tell the outcome of interest in the model and the evaluation

# Ensure 'truth' is a binary factor with levels 0 and 1
test_perf <- tibble(
  truth = factor(cld_train_s$y, levels = c(1,0)),  # Ensure this is binary (0 and 1)
  linear = as.numeric(yhat_enet_linear),       # Continuous predictions from linear model
  logistic = as.numeric(yhat_probs_logistic),  # Probabilities from logistic regression
  random_forest = as.numeric(y_hat_rforest)    # Probabilities from random forest
)

# Compute ROC curves for each model separately
roc_linear <- roc_curve(test_perf, truth, linear)
roc_logistic <- roc_curve(test_perf, truth, logistic)
roc_rf <- roc_curve(test_perf, truth, random_forest)

# Add a 'model' label to each ROC curve
roc_linear <- roc_linear %>% mutate(model = "Linear")
roc_logistic <- roc_logistic %>% mutate(model = "Logistic")
roc_rf <- roc_rf %>% mutate(model = "Random Forest")

# Combine the ROC data from all models
roc_combined <- bind_rows(roc_linear, roc_logistic, roc_rf)

# Plot the combined ROC curves using ggplot
ggplot(roc_combined, aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_path(size = 1.2) +
  geom_abline(lty = 2, color = "gray") +  # Reference line for random guessing
  labs(title = "ROC Curves for Linear, Logistic, and Random Forest Models",
       x = "1 - Specificity",
       y = "Sensitivity") +
  theme_minimal()


```

:::


## b. ROC curve: resampling estimate

Recreate the ROC curve from the penalized logistic regression (logreg) and random forest (rf) models using repeated hold-out data. The following steps will guide you:

- For logreg, use $\alpha=.75$. For rf use *mtry = 2*,  *num.trees = 1000*, and fix any other tuning parameters at your choice. 
- Run the following steps 25 times:
    i. Hold out 500 observations.
    ii. Use the remaining observations to estimate $\lambda$ using 10-fold CV for the logreg model. Don't tune any rf parameters.
    iii. Predict the probability of linkage for the 500 hold-out observations.
    iv. Store the predictions and hold-out labels.
    v. Calculate the AUC. 
- Report the mean AUC and standard error for both models. Compare to the results from part a. 
- Produce two plots showing the 25 ROC curves for each model. 
- Note: by estimating $\lambda$ each iteration, we are incorporating the uncertainty present in estimating that tuning parameter. 
    
::: {.callout-note title="Solution"} 
```{r}
# Load necessary libraries
library(glmnet)         # Penalized logistic regression
library(ranger)         # Random forest
library(yardstick)      # For ROC and AUC calculations
library(ggplot2)        # For plotting
library(dplyr)          # For data manipulation
library(purrr)          # For repeated holdouts

# Set parameters
set.seed(123)           # For reproducibility
alpha <- 0.75           # Elastic net alpha parameter for logistic regression
mtry_rf <- 2            # Random forest mtry parameter
num_trees_rf <- 1000    # Number of trees in random forest
n_iterations <- 25      # Number of iterations
holdout_size <- 500     # Number of hold-out observations per iteration

# Initialize vectors to store AUCs for both models
auc_logreg <- vector("numeric", n_iterations)
auc_rf <- vector("numeric", n_iterations)

# Initialize lists to store ROC curve data
roc_logreg_list <- list()
roc_rf_list <- list()

# Repeat the process 25 times
for (i in 1:n_iterations) {
  
  # Step i: Split data into training and hold-out sets
  holdout_idx <- sample(1:nrow(cld_train_s), holdout_size)
  holdout_data <- cld_train_s[holdout_idx, ]
  training_data <- cld_train_s[-holdout_idx, ]
  
  # Step ii: Logistic regression (penalized logreg with alpha = 0.75)
  X_train <- as.matrix(training_data[, -which(names(training_data) == "y")])
  y_train <- training_data$y
  
  # Perform 10-fold CV to estimate lambda for logistic regression
  cv_logreg <- cv.glmnet(X_train, y_train, alpha = alpha, family = "binomial", nfolds = 10)
  best_lambda <- cv_logreg$lambda.min  # Best lambda from cross-validation
  
  # Step iii: Predict the holdout observations using penalized logreg
  X_holdout <- as.matrix(holdout_data[, -which(names(holdout_data) == "y")])
  y_holdout <- holdout_data$y
  yhat_logreg <- predict(cv_logreg, newx = X_holdout, s = best_lambda, type = "response")[,1]
  
  # Step ii: Random forest (fixed parameters)
  rf_model <- ranger(
    formula = y ~ ., 
    data = training_data, 
    num.trees = num_trees_rf, 
    mtry = mtry_rf, 
    probability = TRUE  # Predict class probabilities
  )
  
  # Step iii: Predict the holdout observations using random forest
  yhat_rf <- predict(rf_model, data = holdout_data)$predictions[, 2]  # Prob of class 1
  
  # Step v: Calculate AUC for both models
  auc_logreg[i] <- roc_auc_vec(as.factor(y_holdout), yhat_logreg) # make a vector as well
  auc_rf[i] <- roc_auc_vec(as.factor(y_holdout), yhat_rf)
  
  # make a dataframe/ tible as above and specify which columns from it to pass in below
  
  # Step vi: Store ROC curve data for plotting
  roc_logreg_list[[i]] <- roc_curve(as.factor(y_holdout), yhat_logreg)
  roc_rf_list[[i]] <- roc_curve(as.factor(y_holdout), yhat_rf)
}

# Calculate mean and standard error for AUCs
mean_auc_logreg <- mean(auc_logreg)
se_auc_logreg <- sd(auc_logreg) / sqrt(n_iterations)

mean_auc_rf <- mean(auc_rf)
se_auc_rf <- sd(auc_rf) / sqrt(n_iterations)

# Print the results
print(paste("Mean AUC for LogReg:", mean_auc_logreg))
print(paste("Standard Error for LogReg:", se_auc_logreg))
print(paste("Mean AUC for RF:", mean_auc_rf))
print(paste("Standard Error for RF:", se_auc_rf))

# Step 4: Combine all ROC curve data into one dataframe
roc_logreg_data <- bind_rows(roc_logreg_list, .id = "iteration") %>% mutate(model = "LogReg")
roc_rf_data <- bind_rows(roc_rf_list, .id = "iteration") %>% mutate(model = "RandomForest")

# Step 5: Plot ROC curves for both models

# Plot ROC curves for Logistic Regression
ggplot(roc_logreg_data, aes(x = 1 - specificity, y = sensitivity, group = iteration)) +
  geom_path(alpha = 0.4, color = "blue") +
  geom_abline(lty = 2, color = "gray") +
  labs(title = "ROC Curves for Penalized Logistic Regression (25 iterations)",
       x = "1 - Specificity",
       y = "Sensitivity") +
  theme_minimal()

# Plot ROC curves for Random Forest
ggplot(roc_rf_data, aes(x = 1 - specificity, y = sensitivity, group = iteration)) +
  geom_path(alpha = 0.4, color = "red") +
  geom_abline(lty = 2, color = "gray") +
  labs(title = "ROC Curves for Random Forest (25 iterations)",
       x = "1 - Specificity",
       y = "Sensitivity") +
  theme_minimal()

```
:::

# Problem 4: Contest

## a. Contest Part 1: Predict the estimated *probability* of linkage. 

Predict the estimated *probability* of linkage for the test data (using any model). 

- Submit a .csv file (ensure comma separated format) named `lastname_firstname_1.csv` that includes the column named **p** that is your estimated posterior probability. We will use automated evaluation, so the format must be exact. 
- You are free to any model (even ones we haven't yet covered in the course).
- You are free to use any data transformation or feature engineering.
- You will receive credit for a proper submission; the top five scores will receive 2 bonus points.     
- Your probabilities will be evaluated with respect to the mean negative Bernoulli log-likelihood (known as the average *log-loss* metric):
$$ 
L = - \frac{1}{M} \sum_{i=1}^m [y_i \log \, \hat{p}_i + (1 - y_i) \log \, (1 - \hat{p}_i)]
$$
where $M$ is the number of test observations, $\hat{p}_i$ is the prediction for the $i$th test observation, and $y_i \in \{0,1\}$ are the true test set labels. 

::: {.callout-note title="Solution"}
Add solution here
:::


## b. Contest Part 2: Predict the *linkage label*. 

Predict the linkages for the test data (using any model). 

- Submit a .csv file (ensure comma separated format) named `lastname_firstname_2.csv` that includes the column named **linkage** that takes the value of 1 for linked pairs and 0 for unlinked pairs. We will use automated evaluation, so the format must be exact. 
- You are free to any model (even ones we haven't yet covered in the course).
- You are free to use any data transformation or feature engineering.
- Your labels will be evaluated based on total cost, where cost is equal to `1*FP + 8*FN`. This implies that False Negatives (FN) are 8 times as costly as False Positives (FP).    
- You will receive credit for a proper submission; the top five scores will receive 2 bonus points. Note: you only will get bonus credit for one of the two contests. 

::: {.callout-note title="Solution"}
Add solution here
:::

