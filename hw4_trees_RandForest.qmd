---
title: "Homework #4: Trees and Random Forest" 
author: "**Ainsley McLaughlin**"
format: ds6030hw-html
---

::: {style="background-color:yellow; color:red; display: block; border-color: black; padding:1em"}
This is an **independent assignment**. Do not discuss or work with classmates.
:::

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
data_dir = 'https://mdporter.github.io/teaching/data/' # data directory
library(tidyverse)    # functions for data manipulation  
library(ranger)       # fast random forest implementation
library(modeldata)    # for the ames housing data
```

# Problem 1: Tree splitting metrics for classification

Consider the Gini index, classification error, and entropy impurity measures in a simple classification setting with two classes. 

Create a single plot that displays each of these quantities as a function of $p_m$, the estimated probability of an observation in node $m$ being from class 1. The x-axis should display $p_m$, ranging from 0 to 1, and the y-axis should display the value of the Gini index, classification error, and entropy.

Personal Notes:
- Misclassification Error: This measures the proportion of observations that do not belong to the
majority class in region Rm. It is minimized when all observations in a node belong to the same class
- The Gini Index measures the likelihood of misclassifying a randomly chosen observation
from the node. It is 0 when all observations in a node belong to the same class and reaches its maximum
when class probabilities are uniform
- Cross-Entropy/Deviance: This measures the amount of uncertainty in the node. It is minimized when
one class probability is 1 (i.e., full certainty about class membership) and is maximized when class
probabilities are evenly distributed. It is proportional to the negative of the multinomial log-likelihood
(and hence deviance)




::: {.callout-note title="Solution"}

```{r}
# lib
library(ggplot2)

# Define a sequence of p_m values ranging from 0 to 1
p_m <- seq(0, 1, length.out = 100)

# Calc Gini index, classification error, and entropy
gini <- 2 * p_m * (1 - p_m)
classification_error <- 1 - pmax(p_m, 1 - p_m)
entropy <- -p_m * log2(p_m) - (1 - p_m) * log2(1 - p_m)

# cases where p_m is 0 or 1 to avoid NaN values in entropy
entropy[is.na(entropy)] <- 0

# a df for plotting
misclass_data <- data.frame(
  p_m = rep(p_m, 3),
  mc = c(gini, classification_error, entropy),
  measure = factor(rep(c("Gini", "Classification Error", "Entropy"), each = 100))
)

# Plot 
ggplot(misclass_data, aes(x = p_m, y = mc, color = measure)) +
  geom_line(size = 1) +
  labs(
    title = "Impurity Measures vs. Probability (p_m)",
    x = "p_m (Probability of being in class 1)",
    y = "Impurity",
    color = "Impurity Measure"
  )

```

:::

# Problem 2: Combining bootstrap estimates

```{r, echo=FALSE}
p_red = c(0.2, 0.25, 0.3, 0.4, 0.4, 0.45, 0.7, 0.85, 0.9, 0.9)
```

Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of $X$, produce the following 10 estimates of $\Pr(\text{Class is Red} \mid X=x)$: $\{`r stringr::str_c(p_red, sep=", ")`\}$.

## a. Majority Vote

ISLR 8.2 describes the *majority vote* approach for making a hard classification from a set of bagged classifiers. What is the final classification for this example using majority voting?

Notes: . For a given test observation, we can record the class predicted by each of the B trees, and take a majority vote: the overall prediction is the most commonly occurring majority
vote class among the B predictions.

::: {.callout-note title="Solution"}
```{r}
p_red <- c(0.2, 0.25, 0.3, 0.4, 0.4, 0.45, 0.7, 0.85, 0.9, 0.9)
class_r_g <- c()  # Initialize an empty vector for the classifications

#loop thru p_red and classify as red or green for each prob
for (i in seq_along(p_red)) {
  if (p_red[i] >= 0.5) {
    class_r_g[i] <- "r"  # If probability >= 0.5, red
  } else {
    class_r_g[i] <- "g"  # If probability < 0.5, green
  }
}

#most frequent class (majority vote)
classification_r_g <- names(which.max(table(class_r_g)))

# Output it
if (classification_r_g == "g") {
  print("The classification is green")
} else {
  print("The classification is red")
}

```
:::

## b. Average Probability

An alternative is to base the final classification on the average probability. What is the final classification for this example using average probability?

::: {.callout-note title="Solution"}
```{r}
# Calculate the average probability
avg_pred <- mean(p_red)

# Classify based on the average probability
if (avg_pred >= 0.5) {
  print("The class is red")
} else {
  print("The class is green")
}


```
:::


# Problem 3: Random Forest Tuning 

Random forest has several tuning parameters that you will explore in this problem. We will use the `ames` housing data from the `modeldata` R package.

There are several R packages for Random Forest. The `ranger::ranger()` function is much faster than `randomForest::randomForest()` so we will use this one.


## a. Random forest (`ranger`) tuning parameters 

List all of the random forest tuning parameters in the `ranger::ranger()` function. You don't need to list the parameters related to computation, special models (e.g., survival, maxstat), or anything that won't impact the predictive performance. 

Indicate the tuning parameters you think will be most important to optimize? 

::: {.callout-note title="Solution"}
```{r}
#help(ranger) # brings up the R documentation on this
```
The parameters for the ranger :: range() function are:
ranger(
  formula = NULL,
  data = NULL,
  num.trees = 500,
  mtry = NULL,
  importance = "none",
  write.forest = TRUE,
  probability = FALSE,
  min.node.size = NULL,
  min.bucket = NULL,
  max.depth = NULL,
  replace = TRUE,
  sample.fraction = ifelse(replace, 1, 0.632),
  case.weights = NULL,
  class.weights = NULL,
  splitrule = NULL,
  num.random.splits = 1,
  alpha = 0.5,
  minprop = 0.1,
  split.select.weights = NULL,
  always.split.variables = NULL,
  respect.unordered.factors = NULL,
  scale.permutation.importance = FALSE,
  local.importance = FALSE,
  regularization.factor = 1,
  regularization.usedepth = FALSE,
  keep.inbag = FALSE,
  inbag = NULL,
  holdout = FALSE,
  quantreg = FALSE,
  time.interest = NULL,
  oob.error = TRUE,
  num.threads = NULL,
  save.memory = FALSE,
  verbose = TRUE,
  node.stats = FALSE,
  seed = NULL,
  dependent.variable.name = NULL,
  status.variable.name = NULL,
  classification = NULL,
  x = NULL,
  y = NULL,
  ...
)

*The important parameters to optimize will likely be: * num.trees, mtry, importance, min.node.size, min.bucket, max.depth, sample.fraction 

*We might possibly want to optimize:* class.weights, num.random.splits, num.threads, save.memory 
:::


## b. Implement Random Forest

Use a random forest model to predict the sales price, `Sale_Price`. Use the default parameters and report the 10-fold cross-validation RMSE (square root of mean squared error). 

::: {.callout-note title="Solution"}

```{r}
# Load the necessary packages
# install.packages("ranger")
# install.packages("modeldata")
library(ranger)
library(modeldata)
library(dplyr)

# Import the Ames housing data
data(ames)
data_a <- data.frame(ames)

# Set seed for reproducibility
set.seed(123)

# Number of observations
n_train <- nrow(data_a)

# Number of folds for cross-validation
n.folds <- 10

# Generate fold assignments for 10-fold cross-validation
set.seed(2021)  # Ensure reproducibility
fold <- sample(rep(1:n.folds, length = n_train))

# Set up a vector to store the RMSE for each fold
rmse_vector <- numeric(n.folds)

# Iterate over each fold for cross-validation
for (j in 1:n.folds) {
  
  # Define the training and validation indices
  val_idx <- which(fold == j)  # Indices for validation set
  train_idx <- which(fold != j)  # Indices for training set
  
  # Split data into training and validation sets
  train_data <- data_a[train_idx, ]
  val_data <- data_a[val_idx, ]
  
  # Fit the random forest model using ranger on the training data
  rf_model <- ranger(Sale_Price ~ ., data = train_data)
  
  # Make predictions on the validation set
  y_hat <- predict(rf_model, val_data)$predictions
  
  # Actual values from the validation set
  y_actual <- val_data$Sale_Price
  
  # Calculate the RMSE for the fold and store it
  rmse_vector[j] <- sqrt(mean((y_actual - y_hat)^2))
}

# Calculate the average RMSE across all folds
cv_rmse <- round(mean(rmse_vector),2)

# Output the average RMSE
print(paste("The rmse is",cv_rmse))

```
:::



## c. Random Forest Tuning

Now we will vary the tuning parameters of `mtry` and `min.bucket` to see what effect they have on performance. 

- Use a range of reasonable `mtry` and `min.bucket` values. 
    - The valid `mtry` values are $\{1,2, \ldots, p\}$ where $p$ is the number of predictor variables. However the default value of `mtry = sqrt(p) =` `r sqrt(ncol(ames)-1) %>% floor()` is usually close to optimal, so you may want to focus your search around those values. 
    - The default `min.bucket=1` will grow deep trees. This will usually work best if there are enough trees. But try some values larger and see how it impacts predictive performance. 
    - Set `num.trees=1000`, which is larger than the default of 500. 
- Use 5 times repeated out-of-bag (OOB) to assess performance. That is, run random forest 5 times for each tuning set, calculate the OOB MSE each time and use the average for the MSE associated with the tuning parameters.
- Use a single plot to show the average MSE as a function of `mtry` and `min.bucket`.
- Report the best tuning parameter combination. 
- Note: random forest is a stochastic model; it will be different every time it runs due to the bootstrap sampling and random selection of features to consider for splitting. Set the random seed to control the uncertainty associated with the stochasticity. 
- Hint: If you use the `ranger` package, the `prediction.error` element in the output is the OOB MSE. 

::: {.callout-note title="Solution"}
```{r}
#num cols =74

# libraries
library(ggplot2)

# Set seed for reproducibility
set.seed(123)

# Get the number of predictor variables (p) -1 bc we are predicting one of the columns
p <- sqrt(ncol(data_a) - 1) %>% floor()  # p is 8 for this data

# Define the tuning grid for mtry and min.bucket
mtry_vals <- p + c(-4, -1, 0, 1, 5)  # Vals around the default sqrt(p)
min_b_vals <- c(1, 3, 5, 7, 9)  # different depths for trees (those numbers are how many values will be in each leaf)

# Funct to run ranger with given mtry and min.bucket, return OOB error
rang_fun <- function(mt_val, mb_val) {
  rf_model <- ranger(Sale_Price ~ .-Sale_Price, data = data_a, 
                     num.trees = 1000, mtry = mt_val, min.node.size = mb_val, 
                     importance = 'none')
  return(rf_model$prediction.error)  # OOB error (MSE)
}

# data frame to store the results
tuning_results <- expand.grid(mtry = mtry_vals, min.bucket = min_b_vals, OOB_MSE = NA)

# Loop through combos of mtry and min.bucket
for (i in 1:nrow(tuning_results)) {
  mt_val <- tuning_results$mtry[i]
  mb_val <- tuning_results$min.bucket[i]
  
  # Run the model 5 times and avg the OOB error
  oob_errors <- numeric(5)
  for (j in 1:5) {
    oob_errors[j] <- rang_fun(mt_val, mb_val)
  }
  
  # Store the avg OOB MSE for this combination
  tuning_results$OOB_MSE[i] <- mean(oob_errors)
}

# Find best tuning parameter combination
best_params <- tuning_results[which.min(tuning_results$OOB_MSE), ]

#print it out
cat("Best mtry:", best_params$mtry, "\nBest min.bucket:", best_params$min.bucket, 
    "\nLowest OOB MSE:", best_params$OOB_MSE, "\n")

```

```{r}
 
# line plot with mtry on the y-axis, MSE on the x-axis, and color by min.bucket
ggplot(tuning_results, aes(x = OOB_MSE, y = mtry, color = as.factor(min.bucket))) +
  geom_line() +
  geom_point() +
  labs(title = "Average MSE by mtry and min.bucket",
       x = "OOB MSE",
       y = "mtry",
       color = "min.bucket") +
  theme_minimal()



```
:::

