---
title: "Homework #2: Resampling" 
author: "**Ainsley McLaughlin**"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```


# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
data_dir = 'https://mdporter.github.io/teaching/data/' # data directory
library(tidymodels)# for optional tidymodels solutions
library(tidyverse) # functions for data manipulation  
```


# Problem 1: Bootstrapping 

Bootstrap resampling can be used to quantify the uncertainty in a fitted curve. 

## a. Data Generating Process

Create a set of functions to generate data from the following distributions:
\begin{align*}
X &\sim \mathcal{U}(0, 2) \qquad \text{Uniform between $0$ and $2$}\\
Y &= 1 + 2x + 5\sin(5x) + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma=2.5)
\end{align*}

::: {.callout-note title="Solution"}

```{r}
X_fun<-function(n){
  runif(n, min = 0, max = 2) #from a uniform with n observations
}

f <- function(x) {
  1 + 2*x + 5*sin(5*x)     # true mean function
}

Y_fun <- function(x){     # generate Y|X from N{f(x),sd}
  n = length(x)
  f(x) + rnorm(n, mean=0, sd=2.5)
}
```

:::

## b. Simulate data

Simulate $n=100$ realizations from these distributions. Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$. Use `set.seed(211)` prior to generating the data.

::: {.callout-note title="Solution"}

```{r}
library(tidyverse)
#set seed for replicability
set.seed(211)
#sim data w 100 obsv
x = X_fun(100)
y = Y_fun(x)
#put into a df (tibble)
data_sim<-tibble(X=x,Y=y)
#View(data_sim)


#plot
library(ggplot2)
ggplot(data_sim, aes(x,y))+
  geom_point()+
  geom_function(fun=f, color="orange")

```

:::


## c. 5th degree polynomial fit

Fit a 5th degree polynomial. Produce a scatterplot and draw the *estimated* regression curve.

::: {.callout-note title="Solution"}

```{r}
#fit the model
fifth_poly<-lm(y ~ poly(x, degree = 5), data = data_sim)

# Create predictions for plotting, add to the df that exists
data_sim$poly5_pred <- predict(fifth_poly, newdata = data_sim)

#plot
ggplot(data_sim, aes(x,y))+
  geom_point()+
  geom_function(fun=f, aes(color = "True_regression"))+
  geom_line(aes(y = poly5_pred, color="Fifth_deg"))+
  labs(title = "Scatterplot with Estimated Regression Lines",
       x = "X",
       y = "Y")+
  scale_color_manual(values=c('Fifth_deg'='blue','True_regression'='orange'))
```

:::


## d. Bootstrap sampling

Make 200 bootstrap samples. For each bootstrap sample, fit a 5th degree polynomial and make predictions at `eval_pts = seq(0, 2, length=100)`

- Set the seed (use `set.seed(212)`) so your results are reproducible.
- Produce a scatterplot with the original data and add the 200 bootstrap curves

::: {.callout-note title="Solution"}


```{r}

set.seed(212)

# Define evaluation points for predictions
eval_pts <- seq(0, 2, length = 100)

# Function for the prediction points and sampling with replacement
pred_samp_fun <- function(train_data, evals) {
  # Sample with replacement at that index
  #boot_samp <- train_data[sample(1:nrow(train_data), nrow(train_data), replace = TRUE), ]
  index=sample(1:nrow(train_data),nrow(train_data), replace = TRUE)
  boot_samp<-train_data[index,]
  # Fit polynomial model
  poly5_boot_fit <- lm(Y ~ poly(X, degree = 5), data = boot_samp)
  # Predict
  predict(poly5_boot_fit, newdata = data.frame(X = evals))
}

# Call this function 200 times and structure the output
boot_preds <- replicate(200, {pred_samp_fun(data_sim, eval_pts)})

# Reshape predictions into a data frame
boot_pred_df <- data.frame(
  X = rep(eval_pts, 200),
  Y = as.vector(boot_preds),
  curve_id = rep(1:200, each = length(eval_pts))
)

# Plot
ggplot() +
  geom_point(data = data_sim, aes(x = X, y = Y), color = "blue") +  # Scatterplot of original data
  geom_line(data = boot_pred_df, aes(x = X, y = Y, group = curve_id), 
            color = "purple", alpha = 0.1) +  # 200 bootstrap prediction curves
  labs(title = "Bootstrap Predictions with 200 bootstrap curves",
       x = "X", y = "Y")
```



:::
    
## e. Confidence Intervals

Calculate the pointwise 95% confidence intervals from the bootstrap samples. That is, for each $x \in {\rm eval\_pts}$, calculate the upper and lower limits such that only 5% of the curves fall outside the interval at $x$. 

- Remake the plot from part *c*, but add the upper and lower boundaries from the 95% confidence intervals. 

::: {.callout-note title="Solution"}

```{r}
#geom_ribbon with the confidence interval dataframe
CI <- apply(boot_preds, 1, quantile, c(0.025, 0.975))
CI_df <- data.frame(
  X = eval_pts,             # Assuming eval_pts is the same x used in boot_preds
  lower = CI[1, ],          # 2.5th percentile
  upper = CI[2, ]           # 97.5th percentile
)

data_sim$poly5_pred <- predict(fifth_poly, newdata = data_sim)
# Plot with confidence intervals
ggplot(data_sim, aes(x = X, y = y)) +
  geom_point() +
  geom_line(aes(y = poly5_pred, color = "Fifth_deg")) +
  geom_ribbon(data = CI_df, aes(x = X, ymin = lower, ymax = upper), alpha = 0.2) +
  labs(title = "Scatterplot with Estimated Regression Line and Confidence Interval",
       x = "X",
       y = "Y") +
  scale_color_manual(values = c('Fifth_deg' = 'blue'))

```

:::

# Problem 2: V-Fold cross-validation with $k$ nearest neighbors

Run 10-fold cross-validation on the data generated in part 1b to select the optimal $k$ in a k-nearest neighbor (kNN) model. Then evaluate how well cross-validation performed by evaluating the performance on a large test set. The steps below will guide you.


## a. Implement 10-fold cross-validation

Use $10$-fold cross-validation to find the value of $k$ (i.e., neighborhood size) that provides the smallest cross-validated MSE using a kNN model. 

- Search over $k=3,4,\ldots, 40$.
- Use `set.seed(221)` prior to generating the folds to ensure the results are replicable. 
- Show the following:
    - the optimal $k$ (as determined by cross-validation)
    - the corresponding estimated MSE
    - produce a plot with $k$ on the x-axis and the estimated MSE on the y-axis (optional: add 1-standard error bars). 
- Notation: The $k$ is the tuning paramter for the kNN model. The $v=10$ is the number of folds in V-fold cross-validation. Don't get yourself confused.

::: {.callout-note title="Solution"}
 Found k fold from cross valnotes but use knn_eval from supervised 2 notes
 

```{r}
# WORKING BETTER
# packages
library(FNN)  # For knn.reg
#library(tibble)
#library(dplyr)
#library(ggplot2)

# Define the knn_eval function
knn_eval <- function(kvalues, data_train, data_test){
  # Initialize the vectors for mse values and edf values
  msevalues <- numeric(length(kvalues))  # Store MSEs
  edfvalues <- numeric(length(kvalues))  # Store effective degrees of freedom
  
  # Loop over the different values of k
  for (i in 1:length(kvalues)){
    knn = knn.reg(data_train[,'X', drop=FALSE], # training predictors
                  y = data_train$Y, # training outcome
                  test = data_test[,'X', drop=FALSE], # testing predictors
                  k = kvalues[i]) # k-neighbors

    r_test = data_test$Y - knn$pred # residuals on test data
    mse_test = mean(r_test^2) # test MSE
    msevalues[i] = mse_test # Store test MSE

    # Effective degrees of freedom
    edf = nrow(data_train) / kvalues[i]
    edfvalues[i] = edf # Store edf
  }
  tibble(k = kvalues, edf = edfvalues, mse_test = msevalues)
}

# Set seed for reproducibility
set.seed(221)

# Simulate data 
x <- X_fun(100)  # Generate X data
y <- Y_fun(x)    # Generate Y data
data_sim <- data.frame(X = x, Y = y)  # Create a dataframe

# 10-fold cross-validation setup
n.folds = 10  # Number of folds for cross-validation
fold = sample(rep(1:n.folds, length = nrow(data_sim)))  # Randomly assign folds

# Initialize a list to store the results for each fold
results = vector("list", n.folds)

# Iterate over folds
for (j in 1:n.folds){
  # Set training and validation data
  val = which(fold == j)  # Validation indices
  train = which(fold != j)  # Training indices
  
  # Fit and evaluate models for k from 3 to 40
  results[[j]] = knn_eval(3:40, # k values from 3 to 40
                          data_train = data_sim[train,], 
                          data_test = data_sim[val,]
  ) %>%
  mutate(fold = j) # Add fold number
}

# Combine results from all folds
RESULTS = bind_rows(results)

# Calculate the average MSE for each k across all folds
mse_k <- RESULTS %>%
  group_by(k) %>%
  summarize(mse = mean(mse_test))

# Plot with k on the x-axis and the estimated MSE on the y-axis
ggplot(mse_k, aes(x = k, y = mse)) +
  geom_line() +
  geom_point() +
  labs(title = "kNN Cross-Validation: k vs. Test MSE",
       x = "k (Neighborhood size)",
       y = "Mean Squared Error (MSE)")
```

```{r}
# Find the optimal k and corresponding MSE
optimal_k <- mse_k %>%
  filter(mse == min(mse))

# Show the optimal k and corresponding estimated MSE
optimal_k


```


:::


## b. Find the optimal *edf*

The $k$ (number of neighbors) in a kNN model determines the effective degrees of freedom *edf*. What is the optimal *edf*? Be sure to use the correct sample size when making this calculation. Produce a plot similar to that from part *a*, but use *edf* (effective degrees of freedom) on the x-axis. 

::: {.callout-note title="Solution"}

```{r}
# edf=n/k
# Group the results by edf and calculate the mean MSE for each edf value
edf_results <- RESULTS %>%
  group_by(edf) %>%
  summarize(mse = mean(mse_test))

# Plot with edf on the x-axis and the mean MSE on the y-axis
ggplot(data = edf_results, aes(x = edf, y = mse)) +
  geom_line() +
  geom_point() +
  labs(title = "kNN Cross-Validation: edf vs. Test MSE",
       x = "Effective Degrees of Freedom (edf)",
       y = "Mean Squared Error (MSE)") +
  theme_minimal()
```
```{r}
# Find the optimal edf corresponding to the minimum MSE
optimal_edf <- edf_results %>%
  filter(mse == min(mse))

# Display the optimal edf and corresponding MSE
optimal_edf

```

:::

## c. Choose $k$

After running cross-validation, a final model fit from *all* of the training data needs to be produced to make predictions. What value of $k$ would you choose? Why? 

::: {.callout-note title="Solution"}

Note: I chose k=9 since this was the k that had the lowest MSE value. The k associated with the lowest MSE effectively balances bias and variance and generally does the best on test data. Low k values make the model more flexible (higher edf), but this can lead to high variance (overfitting). However, high k values have lower edf, but usually increased bias which can potentially result in underfitting. 
:::

## d. Evaluate actual performance

Now we will see how well cross-validation performed. Simulate a test data set of $50000$ observations from the same distributions. Use `set.seed(223)` prior to generating the test data. 

- Fit a set of kNN models, using the full training data, and calculate the mean squared error (MSE) on the test data for each model. Use the same $k$ values in *a*. 
- Report the optimal $k$, the corresponding *edf*, and MSE based on the test set.

::: {.callout-note title="Solution"}

```{r}
# Set seed and generate the test data (replace X_fun and Y_fun with your actual functions)
set.seed(223)
n_test <- 50000
X_test <- X_fun(n_test)  # Simulate X_test data
Y_test <- Y_fun(X_test)  # Simulate Y_test data
test_data <- data.frame(X = X_test, Y = Y_test)

# Define the k values to be used
k_values <- 3:40

# Initialize vector to store MSE for each k
mse_test_values <- numeric(length(k_values))
edf_values <- numeric(length(k_values))

# Loop over k values to fit the kNN model and calculate MSE on test data
for (i in seq_along(k_values)) {
  k <- k_values[i]
  
  # Fit kNN model using the full training data
  knn_fit <- knn.reg(data_sim[,'X', drop=FALSE], 
                     y = data_sim$Y, 
                     test = test_data[,'X', drop=FALSE], 
                     k = k)
  
  # Calculate residuals and MSE on test data
  residuals_test <- test_data$Y - knn_fit$pred
  mse_test_values[i] <- mean(residuals_test^2)
  
  # Calculate edf (effective degrees of freedom)
  edf_values[i] <- nrow(data_sim) / k
}

# Create a summary tibble
test_results <- tibble(k = k_values, edf = edf_values, mse_test = mse_test_values)

# Find the optimal k based on the test set MSE
optimal_test_result <- test_results %>%
  filter(mse_test == min(mse_test))

# Display the optimal k, corresponding edf, and MSE
optimal_test_result


```

:::

## e. Performance plots

Plot both the cross-validation estimated and (true) error calculated from the test data on the same plot. See Figure 5.6 in ISL (pg 182) as a guide. 

- Produce two plots: one with $k$ on the x-axis and one with *edf* on the x-axis.
- Each plot should have two lines: one from part *a* and one from part *d* 

    
::: {.callout-note title="Solution"}
```{r}
# make a new dataframe:
# define kvals again
k_values <- 3:40
# the df
combined_df<-data.frame(
  # k vals
  k=k_values,
  # edf vals
  edf=100/k_values,
  mse_cv=mse_k$mse,
  mse_test=test_results$mse_test)

```

# RETRY PLOT 1:
```{r}
# Plot with k on the x-axis and the estimated MSE on the y-axis
ggplot(combined_df, aes(x = k)) +
  geom_line(aes(y=mse_cv, color="CV MSE")) +
  geom_point(aes(y=mse_cv, color="CV MSE"))+
  geom_line(aes(y=mse_test, color="Test MSE")) +
  geom_point(aes(y=mse_test, color="TEST MSE"))+
  scale_color_manual(name="Type",values=c("CV MSE"="blue","TEST MSE"="red"))
  
```

```{r}
ggplot(combined_df, aes(x = edf)) +
  geom_line(aes(y=mse_cv, color="CV MSE")) +
  geom_point(aes(y=mse_cv, color="CV MSE"))+
  geom_line(aes(y=mse_test, color="Test MSE")) +
  geom_point(aes(y=mse_test, color="TEST MSE"))+
  scale_color_manual(name="Type",values=c("CV MSE"="blue","TEST MSE"="red"))
```

:::
    
## f. Did cross-validation work as intended?

Based on the plots from *e*, does it appear that cross-validation worked as intended? How sensitive is the choice of $k$ on the resulting test MSE?      

::: {.callout-note title="Solution"}

Unfortunately, I believe the plots above are not correct, but here is the logic anyway. If cross-validation worked, the curves for both plots from cross-validation (blue) and the curve from the test set (red) should follow a similar trend. In both plots this is generally (although loosely) true. If both curves minimize around the same k value, then this means the cross-validation identified the k value that minimized the MSE, which then performed well on the test set as expected. This also seems to be loosely true in the above plots. 

The sensitivity of the choice of k on the test MSE can be judged by how steeply the curve changes around the optimal k. A steeper curve indicates high sensitivity, while a flatter curve suggests that multiple values of 
k would perform similarly well (low sensitivity).In our case I would say that the choice of k is sensitive but not extremely so as a few values around the optimal k would produce similar MSE values. 


:::

#Other notes to myself:
Smaller choice of k means less points around to use so is highly sensitive, likely overfit and biased but low variance. Larger k means lots of points used so less sensitive, perhaps underfit, and low bias but higher variance as it not very specific. 


