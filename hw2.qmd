---
title: "Homework #2: Resampling" 
author: "**Ainsley McLaughlin**"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```


# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
data_dir = 'https://mdporter.github.io/teaching/data/' # data directory
library(tidymodels)# for optional tidymodels solutions
library(tidyverse) # functions for data manipulation  
```


# Problem 1: Bootstrapping 

Bootstrap resampling can be used to quantify the uncertainty in a fitted curve. 

## a. Data Generating Process

Create a set of functions to generate data from the following distributions:
\begin{align*}
X &\sim \mathcal{U}(0, 2) \qquad \text{Uniform between $0$ and $2$}\\
Y &= 1 + 2x + 5\sin(5x) + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma=2.5)
\end{align*}

::: {.callout-note title="Solution"}

```{r}
X_fun<-function(n){
  runif(n, min = 0, max = 2) #from a uniform with n observations
}

f <- function(x) {
  1 + 2*x + 5*sin(5*x)     # true mean function
}

Y_fun <- function(x){     # generate Y|X from N{f(x),sd}
  n = length(x)
  f(x) + rnorm(n, mean=0, sd=sqrt(2.5))
}
```

:::

## b. Simulate data

Simulate $n=100$ realizations from these distributions. Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$. Use `set.seed(211)` prior to generating the data.

::: {.callout-note title="Solution"}

```{r}
library(tidyverse)
#set seed for replicability
set.seed(211)
#sim data w 100 obsv
x = X_fun(100)
y = Y_fun(x)
#put into a df (tibble)
data_sim<-tibble(X=x,Y=y)
#View(data_sim)


#plot
library(ggplot2)
ggplot(data_sim, aes(x,y))+
  geom_point()+
  geom_function(fun=f, color="orange")

```

:::


## c. 5th degree polynomial fit

Fit a 5th degree polynomial. Produce a scatterplot and draw the *estimated* regression curve.

::: {.callout-note title="Solution"}

```{r}
#fit the model
fifth_poly<-lm(y ~ poly(x, degree = 5), data = data_sim)

# Create predictions for plotting, add to the df that exists
data_sim$poly5_pred <- predict(fifth_poly, newdata = data_sim)

#plot
ggplot(data_sim, aes(x,y))+
  geom_point()+
  geom_function(fun=f, aes(color = "True_regression"))+
  geom_line(aes(y = poly5_pred, color="Fifth_deg"))+
  labs(title = "Scatterplot with Estimated Regression Lines",
       x = "X",
       y = "Y")+
  scale_color_manual(values=c('Fifth_deg'='blue','True_regression'='orange'))
```

:::


## d. Bootstrap sampling

Make 200 bootstrap samples. For each bootstrap sample, fit a 5th degree polynomial and make predictions at `eval_pts = seq(0, 2, length=100)`

- Set the seed (use `set.seed(212)`) so your results are reproducible.
- Produce a scatterplot with the original data and add the 200 bootstrap curves

::: {.callout-note title="Solution"}


Another way------------------

# july 16th in princes class we did this
# Set the seed for reproducibility
set.seed(212)

# Number of bootstrap samples
n_boot = 200

# Define evaluation points for predictions
eval_pts = seq(0, 2, length = 100)

# Initialize an empty tibble to store predictions
preds <- list()

# Loop over the number of bootstrap samples
for (i in 1:n_boot) { # basically from 1 to 200
  
  # Sample from the original training data with replacement
  ind = sample(1:nrow(data_sim), replace = TRUE) # Sample row indices
  data_boot = data_sim[ind, ] # Create bootstrap sample
  
  # Fit 5th-degree polynomial model
  m_boot = lm(y ~ poly(x, degree = 5), data = data_boot) # Fit polynomial model
  
  # Predict values at the evaluation points
  poly5_pred = predict(m_boot, newdata = data.frame(x=eval_pts))
  
  # Store the predictions in the preds tibble
  preds[[i]]<-data.frame(x=eval_pts, y=poly5_pred, iteration=i)
  
  preds_data <- bind_rows(preds)
}

# Visualize: Scatterplot with original data and 200 bootstrap curves
ggplot(data_sim, aes(x = x, y = y)) +
  geom_point(color = "blue") +  # Scatterplot of original data
  geom_line(data = preds_data, aes(x = x, y = y, group = iteration), 
            color = "red", alpha = 0.4) +  # 200 bootstrap prediction curves
  labs(title = "Bootstrap Predictions with 200 bootstrap curves",
       x = "X", y = "Y")

Another wayyyy---------------
# chat's edits which still do not work: 
# Set the seed for reproducibility
set.seed(212)

# Number of bootstrap samples
n_boot = 200

# Define evaluation points for predictions
eval_pts = data.frame(x = seq(0, 2, length = 100)) # Create a data frame for eval_pts

# Initialize an empty list to store predictions
preds <- vector("list", n_boot)

# Loop over the number of bootstrap samples
for (i in 1:n_boot) {
  
  # Sample from the original training data with replacement
  ind = sample(1:nrow(data_sim), replace = TRUE) # Sample row indices
  data_boot = data_sim[ind, ] # Create bootstrap sample
  
  # Fit 5th-degree polynomial model
  m_boot = lm(y ~ poly(x, degree = 5), data = data_boot) # Fit polynomial model
  
  # Predict values at the evaluation points
  poly5_pred = predict(m_boot, newdata = eval_pts)
  
  # Store the predictions in the preds list, assigning iteration as i
  preds[[i]] <- data.frame(x = eval_pts$x, y = poly5_pred, iteration = i)
}

# Combine all predictions into a single data frame
preds_data <- bind_rows(preds)

# Visualize: Scatterplot with original data and 200 bootstrap curves
ggplot(data_sim, aes(x = x, y = y)) +
  geom_point(color = "blue") +  # Scatterplot of original data
  geom_line(data = preds_data, aes(x = x, y = y, group = iteration), 
            color = "red", alpha = 0.2) +  # 200 bootstrap prediction curves
  labs(title = "Bootstrap Predictions with 200 bootstrap curves",
       x = "X", y = "Y")

-----------------
# ANOTHER WAY
# try with the function replicated instead:
# Set the seed for reproducibility
set.seed(212)

# Number of bootstrap samples
n_boot = 200

# Define evaluation points for predictions
eval_pts = seq(0, 2, length = 100) 
# function for the prediction points and sampling with replacement
pred_samp_fun<-function(train_data, evals){
  # sample with replacement at that index
  boot_samp = train_data[sample(1:nrow(train_data), replace = TRUE),]
  # Fit polynomial model
  poly5_boot_fit = lm(y ~ poly(x, degree = 5), data = boot_samp) 
  #predict
  predict(poly5_boot_fit, newdata = data.frame(x=evals))
  
}
# call this function 200 times:
boot_preds<-replicate(200, pred_samp_fun(data_sim, eval_pts))
boot_pred_df<-data.frame(boot_preds)

# plot
ggplot(data_sim, aes(x = x, y = y)) +
  geom_point(color = "blue") +  # Scatterplot of original data
  geom_line(data = boot_pred_df, aes(x = x, y = y), 
            color = "green", alpha = 0.2) +  # 200 bootstrap prediction curves
  labs(title = "Bootstrap Predictions with 200 bootstrap curves",
       x = "X", y = "Y")


```{r}

set.seed(212)

# Define evaluation points for predictions
eval_pts <- seq(0, 2, length = 100)

# Function for the prediction points and sampling with replacement
pred_samp_fun <- function(train_data, evals) {
  # Sample with replacement at that index
  boot_samp <- train_data[sample(1:nrow(train_data), nrow(train_data), replace = TRUE), ]
  # Fit polynomial model
  poly5_boot_fit <- lm(y ~ poly(x, degree = 5), data = boot_samp)
  # Predict
  predict(poly5_boot_fit, newdata = data.frame(x = evals))
}

# Call this function 200 times and structure the output
boot_preds <- replicate(200, {pred_samp_fun(data_sim, eval_pts)})

# Reshape predictions into a data frame
boot_pred_df <- data.frame(
  X = rep(eval_pts, 200),
  Y = as.vector(boot_preds),
  curve_id = rep(1:200, each = length(eval_pts))
)

# Plot
ggplot() +
  geom_point(data = data_sim, aes(x = x, y = y), color = "blue") +  # Scatterplot of original data
  geom_line(data = boot_pred_df, aes(x = X, y = Y, group = curve_id), 
            color = "purple", alpha = 0.1) +  # 200 bootstrap prediction curves
  labs(title = "Bootstrap Predictions with 200 bootstrap curves",
       x = "X", y = "Y")
```
```{r}
# chat's answer:

# Set the seed for reproducibility
set.seed(212)

# Define evaluation points for predictions
eval_pts <- seq(0, 2, length = 100)

# Function for the prediction points and sampling with replacement
pred_samp_fun <- function(train_data, evals) {
  # Sample with replacement (check this)
  boot_samp <- train_data[sample(1:nrow(train_data), nrow(train_data), replace = TRUE), ]
  
  # Fit polynomial model (check model variability)
  poly5_boot_fit <- lm(y ~ poly(x, degree = 5), data = boot_samp)
  
  # Return predicted values
  predict(poly5_boot_fit, newdata = data.frame(x = evals))
}

# Generate predictions for 200 bootstrap samples
boot_preds <- replicate(200, pred_samp_fun(data_sim, eval_pts))

# Check if the first few bootstrap samples are different
print(boot_preds[, 1:5])  # Inspect first 5 samples for variability

# Reshape predictions into a data frame
boot_pred_df <- data.frame(
  X = rep(eval_pts, 200),
  Y = as.vector(boot_preds),
  curve_id = rep(1:200, each = length(eval_pts))
)

# Plot with ggplot
ggplot() +
  geom_point(data = data_sim, aes(x = x, y = y), color = "blue") +  # Scatterplot of original data
  geom_line(data = boot_pred_df, aes(x = X, y = Y, group = curve_id), 
            color = "purple", alpha = 0.1) +  # 200 bootstrap prediction curves
  labs(title = "Bootstrap Predictions with 200 Curves",
       x = "X", y = "Y") +
  theme_minimal()


```
```{r}
# when prince did it: # Bootstrapping:
take avg of all estimates for Bo and B1. Is more robust than just using lm to estimate the parameters

```{r}
#do this code 5 times
Boot_estimates<-replicate(5,{

#the sample takes random samples and choose the rows from 1 to the number of rows in the dataset, how many times you want to sample it is in the second arg, also replace=TRUE, leave so selects all the columns
boot_samps<-st[sample(1:nrow(st),nrow(st),replace=T),]
#now everytime pick a sample, fit a linear model
boot_model<-lm(Profit~R.D.Spend, data=boot_samps)
#get the coeff of that model
coef(boot_model)
})

#the bootstrapp dataset is not a tidy dataset yet:transpose it and coerce to a df
estimates_df<-data.frame(t(Boot_estimates))
#View(estimates)

#now do the graph:
ggplot(st, aes(x=R.D.Spend, y=Profit))+geom_point()+
  geom_smooth(method='lm', se=F, color='blue')+
  geom_abline(data=estimates_df, aes(intercept=X.Intercept., slope=R.D.Spend),color="pink")
#abline will draw a line thru intercept and pt

```

:::
    
## e. Confidence Intervals

Calculate the pointwise 95% confidence intervals from the bootstrap samples. That is, for each $x \in {\rm eval\_pts}$, calculate the upper and lower limits such that only 5% of the curves fall outside the interval at $x$. 

- Remake the plot from part *c*, but add the upper and lower boundaries from the 95% confidence intervals. 

::: {.callout-note title="Solution"}

```{r}

```

:::

# Problem 2: V-Fold cross-validation with $k$ nearest neighbors

Run 10-fold cross-validation on the data generated in part 1b to select the optimal $k$ in a k-nearest neighbor (kNN) model. Then evaluate how well cross-validation performed by evaluating the performance on a large test set. The steps below will guide you.


## a. Implement 10-fold cross-validation

Use $10$-fold cross-validation to find the value of $k$ (i.e., neighborhood size) that provides the smallest cross-validated MSE using a kNN model. 

- Search over $k=3,4,\ldots, 40$.
- Use `set.seed(221)` prior to generating the folds to ensure the results are replicable. 
- Show the following:
    - the optimal $k$ (as determined by cross-validation)
    - the corresponding estimated MSE
    - produce a plot with $k$ on the x-axis and the estimated MSE on the y-axis (optional: add 1-standard error bars). 
- Notation: The $k$ is the tuning paramter for the kNN model. The $v=10$ is the number of folds in V-fold cross-validation. Don't get yourself confused.

::: {.callout-note title="Solution"}


```{r}
# from chat:
# Load necessary libraries
library(caret)  # For cross-validation and training models
library(ggplot2)  # For plotting results

# Set seed for reproducibility
set.seed(221)

# Simulate data (replace with your actual data if needed)
x <- X_fun(100)
y <- Y_fun(x)
data_sim <- data.frame(X = x, Y = y)

# Prepare the training control for 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Define the range of k values to search over
k_values <- 3:40

# Train the kNN model with cross-validation
knn_model <- train(
  X ~ Y,                     # Formula (X as the predictor, Y as the response)
  data = data_sim,            # Data
  method = "knn",             # Specify kNN method
  tuneGrid = expand.grid(k = k_values),  # Search over k = 3 to 40
  trControl = train_control,  # 10-fold cross-validation
  metric = "RMSE"             # Use RMSE (Root Mean Squared Error) for evaluation
)

# Print optimal k and corresponding MSE (squared RMSE)
optimal_k <- knn_model$bestTune$k
optimal_rmse <- knn_model$results[knn_model$results$k == optimal_k, "RMSE"]
optimal_mse <- optimal_rmse^2

cat("Optimal k:", optimal_k, "\n")
cat("Corresponding MSE:", optimal_mse, "\n")
```

```{r}
# Create a plot of k vs. cross-validated MSE
mse_results <- knn_model$results
mse_results$MSE <- mse_results$RMSE^2  # Convert RMSE to MSE

ggplot(mse_results, aes(x = k, y = MSE)) +
  geom_line() +
  geom_point() +
  labs(title = "kNN Cross-Validation Results",
       x = "Number of Neighbors (k)",
       y = "Cross-Validated MSE") +
  theme_minimal()

```

:::


## b. Find the optimal *edf*

The $k$ (number of neighbors) in a kNN model determines the effective degrees of freedom *edf*. What is the optimal *edf*? Be sure to use the correct sample size when making this calculation. Produce a plot similar to that from part *a*, but use *edf* (effective degrees of freedom) on the x-axis. 

::: {.callout-note title="Solution"}

```{r}
# Load necessary libraries
library(caret)  # For cross-validation and training models
library(ggplot2)  # For plotting

# Set seed for reproducibility
set.seed(221)

# Simulate data (replace with your actual data if needed)
x <- X_fun(100)
y <- Y_fun(x)
data_sim <- data.frame(X = x, Y = y)

# Number of observations
n <- nrow(data_sim)

# Prepare the training control for 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Define the range of k values to search over
k_values <- 3:40

# Train the kNN model with cross-validation
knn_model <- train(
  X ~ Y,                     # Formula (X as the predictor, Y as the response)
  data = data_sim,            # Data
  method = "knn",             # Specify kNN method
  tuneGrid = expand.grid(k = k_values),  # Search over k = 3 to 40
  trControl = train_control,  # 10-fold cross-validation
  metric = "RMSE"             # Use RMSE (Root Mean Squared Error) for evaluation
)

# Optimal k and its corresponding RMSE (Root Mean Squared Error)
optimal_k <- knn_model$bestTune$k
optimal_rmse <- knn_model$results[knn_model$results$k == optimal_k, "RMSE"]
optimal_mse <- optimal_rmse^2

cat("Optimal k:", optimal_k, "\n")
cat("Corresponding MSE:", optimal_mse, "\n")

# Calculate Effective Degrees of Freedom (EDF) for each k
mse_results <- knn_model$results
mse_results$EDF <- n / mse_results$k  # EDF = n / k
mse_results$MSE <- mse_results$RMSE^2  # Convert RMSE to MSE
```

```{r}
# Plot EDF vs MSE
ggplot(mse_results, aes(x = EDF, y = MSE)) +
  geom_line() +
  geom_point() +
  labs(title = "kNN Cross-Validation Results (EDF vs MSE)",
       x = "Effective Degrees of Freedom (EDF)",
       y = "Cross-Validated MSE") +
  theme_minimal()

```

:::

## c. Choose $k$

After running cross-validation, a final model fit from *all* of the training data needs to be produced to make predictions. What value of $k$ would you choose? Why? 

::: {.callout-note title="Solution"}

```{r}
# Refit the final kNN model using the entire dataset with the optimal k
final_knn_model <- train(
  X ~ Y,                   # Formula (X as predictor, Y as response)
  data = data_sim,          # Entire dataset
  method = "knn",           # kNN model
  tuneGrid = expand.grid(k = optimal_k),  # Use the optimal k from cross-validation
  trControl = trainControl(method = "none")  # No cross-validation for the final fit
)

# Make predictions using the final model
final_predictions <- predict(final_knn_model, newdata = test_data)

# Evaluate final model performance on test data (optional)
final_mse <- mean((final_predictions - test_data$Y)^2)
cat("Final Model MSE on Test Data:", final_mse, "\n")


```
Note: I chose k=7 since this was the k that had the lowest MSE value. The k associated with the lowest MSE effectively balances bias and variance and generally does the best on test data. Low k values make the model more flexible (higher edf), but this can lead to high variance (overfitting). However, high k values have lower edf, but usually increased bias which can potentially result in underfitting. 
:::

## d. Evaluate actual performance

Now we will see how well cross-validation performed. Simulate a test data set of $50000$ observations from the same distributions. Use `set.seed(223)` prior to generating the test data. 

- Fit a set of kNN models, using the full training data, and calculate the mean squared error (MSE) on the test data for each model. Use the same $k$ values in *a*. 
- Report the optimal $k$, the corresponding *edf*, and MSE based on the test set.

::: {.callout-note title="Solution"}

```{r}
# Load necessary libraries
library(caret)

# Set seed for reproducibility
set.seed(223)

# Simulate test data with 50,000 observations from the same distribution
test_x <- X_fun(50000)
test_y <- Y_fun(test_x)
test_data <- data.frame(X = test_x, Y = test_y)

# Set seed for training data generation (if not done already)
set.seed(211)
train_x <- X_fun(100)
train_y <- Y_fun(train_x)
train_data <- data.frame(X = train_x, Y = train_y)

# Number of observations in the training data
n_train <- nrow(train_data)

# Define the range of k values (from part a)
k_values <- 3:40

# Initialize a data frame to store results
results <- data.frame(k = k_values, MSE = NA, EDF = NA)

# Fit models for each k and calculate MSE on test data
for (k in k_values) {
  # Train kNN model with the full training data
  knn_model <- train(
    X ~ Y,                    # Formula
    data = train_data,         # Full training data
    method = "knn",            # Specify kNN method
    tuneGrid = expand.grid(k = k),  # Use specific k
    trControl = trainControl(method = "none")  # No cross-validation
  )
  
  # Predict on the test data
  test_predictions <- predict(knn_model, newdata = test_data)
  
  # Calculate MSE on the test data
  mse <- mean((test_predictions - test_data$Y)^2)
  
  # Store the results
  results[results$k == k, "MSE"] <- mse
  results[results$k == k, "EDF"] <- n_train / k  # Effective Degrees of Freedom
}

# Find the optimal k (the one with the lowest MSE)
optimal_k_test <- results[which.min(results$MSE), "k"]
optimal_mse_test <- results[which.min(results$MSE), "MSE"]
optimal_edf_test <- results[which.min(results$MSE), "EDF"]

# Print the results
cat("Optimal k on Test Set:", optimal_k_test, "\n")
cat("Corresponding EDF:", optimal_edf_test, "\n")
cat("Test Set MSE:", optimal_mse_test, "\n")


```

:::

## e. Performance plots

Plot both the cross-validation estimated and (true) error calculated from the test data on the same plot. See Figure 5.6 in ISL (pg 182) as a guide. 

- Produce two plots: one with $k$ on the x-axis and one with *edf* on the x-axis.
- Each plot should have two lines: one from part *a* and one from part *d* 

    
::: {.callout-note title="Solution"}


```{r}
# Load necessary libraries
library(ggplot2)

# PART A: Results from cross-validation
# Assuming you have already trained the kNN model and have 'knn_model' from part a
# knn_model$results contains the cross-validation results

# Create a data frame with k values, cross-validated RMSE, and MSE
cv_results <- knn_model$results
cv_results$MSE <- cv_results$RMSE^2  # Square RMSE to get MSE
cv_results$EDF <- nrow(train_data) / cv_results$k  # Effective degrees of freedom

# PART D: Test set results (from the loop you shared)
test_results <- results  # Contains k, MSE, and EDF for the test set

# Combine both results for plotting
combined_results <- merge(cv_results, test_results, by = "k", suffixes = c("_cv", "_test"))

# PLOT 1: MSE vs. k
ggplot(combined_results, aes(x = k)) +
  geom_line(aes(y = MSE_cv, color = "Cross-Validation MSE"), linewidth = 1) +
  geom_line(aes(y = MSE_test, color = "Test Set MSE"), linewidth = 1, linetype = "dashed") +
  geom_point(aes(y = MSE_cv), color = "orange", shape = 4, size = 3) +  # Cross for min CV MSE
  geom_point(aes(y = MSE_test), color = "blue", shape = 4, size = 3) +  # Cross for min Test MSE
  labs(title = "MSE vs. k", x = "k (Number of Neighbors)", y = "Mean Squared Error") +
  scale_color_manual(values = c("Cross-Validation MSE" = "orange", "Test Set MSE" = "blue")) +
  theme_minimal()
```

```{r}
# PLOT 2: MSE vs. EDF (Effective Degrees of Freedom)
ggplot(combined_results, aes(x = EDF_cv)) +
  geom_line(aes(y = MSE_cv, color = "Cross-Validation MSE"), linewidth = 1) +
  geom_line(aes(y = MSE_test, color = "Test Set MSE"), linewidth = 1, linetype = "dashed") +
  geom_point(aes(y = MSE_cv), color = "orange", shape = 4, size = 3) +  # Cross for min CV MSE
  geom_point(aes(y = MSE_test), color = "blue", shape = 4, size = 3) +  # Cross for min Test MSE
  labs(title = "MSE vs. EDF", x = "Effective Degrees of Freedom (EDF)", y = "Mean Squared Error") +
  scale_color_manual(values = c("Cross-Validation MSE" = "orange", "Test Set MSE" = "blue")) +
  theme_minimal()



```

:::
    
## f. Did cross-validation work as intended?

Based on the plots from *e*, does it appear that cross-validation worked as intended? How sensitive is the choice of $k$ on the resulting test MSE?      

::: {.callout-note title="Solution"}

Add solution here

:::




